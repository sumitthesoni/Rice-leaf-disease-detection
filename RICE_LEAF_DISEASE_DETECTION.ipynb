{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import notebook \n",
    "import optuna\n",
    "tqdm=notebook.tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting device to cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='D:\\Projects\\Deep Learning\\Rice leaf\\Data'\n",
    "images_paths=[]\n",
    "labels=[]\n",
    "subfolders_keys=os.listdir(path)\n",
    "for lab,subfolder in enumerate(subfolders_keys):\n",
    "    subfolder_path=os.path.join(path,subfolder)\n",
    "    list_images=os.listdir(subfolder_path)\n",
    "    images_paths.extend([os.path.join(subfolder_path,img) for img in list_images])\n",
    "    labels.extend([lab]*len(list_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "random_images=random.sample(images_paths,32)\n",
    "random.seed(42)\n",
    "random_labels=random.sample(labels,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of images to display\n",
    "num_images = len(random_images)\n",
    "cols = 4  # Number of columns\n",
    "rows = (num_images // cols) + 1  # Number of rows\n",
    "\n",
    "plt.figure(figsize=(15, rows * 3))\n",
    "\n",
    "for i, img_path in enumerate(random_images):\n",
    "    img = cv.imread(img_path)  # Read image\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Label: {random_labels[i]}\")\n",
    "    plt.axis(\"off\")  # Hide axes\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(images_paths,labels,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer=transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomRotation(90),        # Rotate image randomly by Â±30 degrees\n",
    "        transforms.RandomHorizontalFlip(),    # Flip image horizontally\n",
    "        transforms.RandomVerticalFlip(),      # Flip image vertically\n",
    "        transforms.RandomPerspective(distortion_scale=0.2, p=0.5),  # Apply perspective transform\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translation\n",
    "        transforms.ToTensor(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Custom Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomData(Dataset):\n",
    "    def __init__(self,X,y,transformer):\n",
    "        self.transform=transformer\n",
    "        self.X=X\n",
    "        self.y=torch.tensor(y,dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, index):\n",
    "        img=Image.open(self.X[index]).convert('RGB')\n",
    "        img=self.transform(img)\n",
    "        return img,self.y[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=CustomData(X_train,y_train)\n",
    "test_data=CustomData(X_test,y_test)\n",
    "train_loader=DataLoader(train_data,batch_size=10,shuffle=True,pin_memory=True)\n",
    "test_loader=DataLoader(test_data,batch_size=10,shuffle=False,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Dynamic CNN Architecture usin optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_conv_layers, kernel_size, stride, dropout_rate, fc_layers, use_batchnorm, num_classes=3):\n",
    "        super(CNN, self).__init__()\n",
    "        layers = []\n",
    "        self.conv_layers = nn.Sequential()  # Store conv layers separately\n",
    "\n",
    "        # Adding convolutional layers\n",
    "        for i in range(num_conv_layers):\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=1))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout2d(dropout_rate))\n",
    "            layers.append(nn.MaxPool2d(2, 2))  # Pooling layer\n",
    "            in_channels = out_channels\n",
    "            out_channels *= 2  # Double channels each time\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Determine the FC input size dynamically\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.randn(1, 3, 224, 224)  # A dummy batch of size 1\n",
    "            conv_output = self.conv_layers(sample_input)\n",
    "            fc_input_dim = conv_output.view(1, -1).shape[1]  # Compute flattened size dynamically\n",
    "\n",
    "        fc_layers_list = []\n",
    "        for i in range(fc_layers):\n",
    "            fc_layers_list.append(nn.Linear(fc_input_dim, 128))\n",
    "            if use_batchnorm:\n",
    "                fc_layers_list.append(nn.BatchNorm1d(128))\n",
    "            fc_layers_list.append(nn.ReLU())\n",
    "            fc_layers_list.append(nn.Dropout(dropout_rate))\n",
    "            fc_input_dim = 128\n",
    "        \n",
    "        fc_layers_list.append(nn.Linear(128, num_classes))  # Output layer\n",
    "\n",
    "        self.fc_layers = nn.Sequential(*fc_layers_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "\n",
    "    in_channels = 3\n",
    "\n",
    "    # Hyperparameters to tune\n",
    "    out_channels=trial.suggest_int(\"out_channels\",8,64)\n",
    "    num_conv_layers = trial.suggest_int(\"num_conv_layers\", 2, 5)\n",
    "    kernel_size = trial.suggest_int(\"kernel_size\", 2,3)\n",
    "    stride = trial.suggest_int(\"stride\", 1, 2)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.5)\n",
    "    fc_layers = trial.suggest_int(\"fc_layers\", 1, 3)\n",
    "    use_batchnorm = trial.suggest_categorical(\"use_batchnorm\", [True, False])\n",
    "    epochs=trial.suggest_int(\"epochs\",20,60)\n",
    "\n",
    "    # Create model\n",
    "    model = CNN(in_channels,out_channels,num_conv_layers,kernel_size,stride,dropout_rate,fc_layers,use_batchnorm).to(device)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop \n",
    "    for i in tqdm(range(epochs),desc='Training',leave=True):\n",
    "        avg_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Train\n",
    "        model.train()\n",
    "        for bat_feat, bat_lab in train_loader:\n",
    "            bat_feat, bat_lab = bat_feat.to(device), bat_lab.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            out = model(bat_feat)\n",
    "\n",
    "            # Loss calculation\n",
    "            loss = loss_fn(out, bat_lab)\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            # Accuracy Calculation\n",
    "            _, predicted = torch.max(out, 1)  # Get predicted class\n",
    "            correct += (predicted == bat_lab).sum().item()  # Count correct predictions\n",
    "            total += bat_lab.size(0)  # Total samples\n",
    "\n",
    "        epoch_loss = avg_loss / len(train_loader)\n",
    "        train_epoch_acc = 100 * correct / total  # Accuracy percentage\n",
    "\n",
    "\n",
    "        # prediction\n",
    "        model.eval()\n",
    "        net_test_accuracy=0\n",
    "        test_epoch=30\n",
    "        with torch.no_grad():\n",
    "            for j in range(test_epoch):\n",
    "                test_correct=0\n",
    "                test_accuracy=0\n",
    "                test_total=0\n",
    "                for bat_feat, bat_lab in test_loader:\n",
    "                    bat_feat, bat_lab = bat_feat.to(device), bat_lab.to(device)\n",
    "                    # Forward pass\n",
    "                    out = model(bat_feat)\n",
    "                    test_correct+=sum(bat_lab==out.argmax(1)).item()\n",
    "                    test_total+=bat_lab.size(0)\n",
    "                test_accuracy += (100 * test_correct / test_total ) # Accuracy percentage\n",
    "                net_test_accuracy+=test_accuracy\n",
    "        print(f'Epoch : {i+1} | Loss : {epoch_loss:.4f} | Train Accuracy: {train_epoch_acc:.2f}% | Test Accuracy : {net_test_accuracy/test_epoch}')\n",
    "    return (net_test_accuracy/test_epoch)  # Optuna maximizes accuracy\n",
    "\n",
    "# Run the optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Xception model (pretrained on ImageNet)\n",
    "model = timm.create_model(\"xception\", pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model,input_size=(1,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers before block 10\n",
    "cond=False\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = cond\n",
    "    if \"block8\" in name:\n",
    "        cond=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc=nn.Sequential(\n",
    "    nn.Linear(2048,128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(128,32),\n",
    "    nn.BatchNorm1d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(32,len(subfolders_keys))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining Transformer\n",
    "transformer = transforms.Compose([\n",
    "        transforms.Resize((299, 299)),  # Xception input size\n",
    "        transforms.RandomRotation(90),        # Rotate image randomly by Â±30 degrees\n",
    "        transforms.RandomHorizontalFlip(),    # Flip image horizontally\n",
    "        transforms.RandomVerticalFlip(),      # Flip image vertically\n",
    "        transforms.RandomPerspective(distortion_scale=0.2, p=0.5),  # Apply perspective transform\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translation\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomData(Dataset):\n",
    "    def __init__(self,X,y,transformer):\n",
    "        self.transform=transformer\n",
    "        self.X=X\n",
    "        self.y=torch.tensor(y,dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, index):\n",
    "        img=Image.open(self.X[index]).convert('RGB')\n",
    "        img=self.transform(img)\n",
    "        return img,self.y[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=CustomData(X_train,y_train,transformer)\n",
    "test_data=CustomData(X_test,y_test,transformer)\n",
    "train_loader=DataLoader(train_data,batch_size=10,shuffle=True,pin_memory=True)\n",
    "test_loader=DataLoader(test_data,batch_size=10,shuffle=False,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalizing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(device)\n",
    "optimizer=optim.SGD(model.parameters(),lr=0.01,weight_decay=0.01)\n",
    "loss_fn=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Display Summary\n",
    "summary(model,input_size=(1,3,229,229))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=[]\n",
    "train_accuracy=[]\n",
    "general_test_accuracy=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(41),desc='Training',leave=True):\n",
    "    avg_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Train\n",
    "    model.train()\n",
    "    for bat_feat, bat_lab in train_loader:\n",
    "        bat_feat, bat_lab = bat_feat.to(device), bat_lab.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(bat_feat)\n",
    "\n",
    "        # Loss calculation\n",
    "        loss = loss_fn(out, bat_lab)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        # Accuracy Calculation\n",
    "        _, predicted = torch.max(out, 1)  # Get predicted class\n",
    "        correct += (predicted == bat_lab).sum().item()  # Count correct predictions\n",
    "        total += bat_lab.size(0)  # Total samples\n",
    "\n",
    "    epoch_loss = avg_loss / len(train_loader)\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_epoch_acc = 100 * correct / total  # Accuracy percentage\n",
    "    train_accuracy.append(train_epoch_acc)\n",
    "\n",
    "\n",
    "    # prediction\n",
    "    model.eval()\n",
    "    net_test_accuracy=0\n",
    "    test_epoch=30\n",
    "    with torch.no_grad():\n",
    "        for j in range(test_epoch):\n",
    "            test_correct=0\n",
    "            test_accuracy=0\n",
    "            test_total=0\n",
    "            for bat_feat, bat_lab in test_loader:\n",
    "                bat_feat, bat_lab = bat_feat.to(device), bat_lab.to(device)\n",
    "                # Forward pass\n",
    "                out = model(bat_feat)\n",
    "                test_correct+=sum(bat_lab==out.argmax(1)).item()\n",
    "                test_total+=bat_lab.size(0)\n",
    "            test_accuracy += (100 * test_correct / test_total ) # Accuracy percentage\n",
    "            net_test_accuracy+=test_accuracy\n",
    "    general_test_accuracy.append(net_test_accuracy/test_epoch)\n",
    "    print(f'Epoch : {i+1} | Loss : {epoch_loss:.4f} | Train Accuracy: {train_epoch_acc:.2f}% | Test Accuracy : {net_test_accuracy/test_epoch}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(range(1, 42), train_loss, label=\"Train Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(range(1, 42), train_accuracy, label=\"Train Accuracy\")\n",
    "plt.plot(range(1, 42), general_test_accuracy, label=\"Test Accuracy\")  # Ensure same x-range\n",
    "plt.legend()  # No argument needed if labels are set in `plt.plot()`\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training vs Test Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Define the number of images to display\n",
    "    num_images = len(random_images)\n",
    "    cols = 4  # Number of columns\n",
    "    rows = (num_images // cols) + 1  # Number of rows\n",
    "\n",
    "    plt.figure(figsize=(15, rows * 3))\n",
    "\n",
    "    for i, img_path in enumerate(random_images):\n",
    "        # prediction\n",
    "        img = Image.open(img_path)  # Read image\n",
    "        # image Prediction\n",
    "        img_pred=transformer(img).unsqueeze(0).to(device)\n",
    "        predicted=model(img_pred).argmax(1).item()\n",
    "        #plotting\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Actual: {subfolders_keys[random_labels[i]]}  | Predicted : {subfolders_keys[predicted]}\",fontdict={'size':8})\n",
    "        plt.axis(\"off\")  # Hide axes\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"_rice_leaf_xception_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "* The project involved building a deep learning model to classify images of rice leaf disease. The\n",
    "  dataset contained images from 3 type of rice leaf disease namely bacterial bligh, brown spot, and leaf\n",
    "  smut, The project was divided into several steps, including data exploration, data preprocessing,\n",
    "  building and training deep learning models, and evaluating model performance.\n",
    "\n",
    "* During data exploration, I analyzed the dataset and visualized the images to get a better under-\n",
    "  standing of the data. I observed that the dataset was balanced, with an equal number of images\n",
    "  for rice leaf diseases. I also noticed that the images were of different sizes and needed to be\n",
    "  resized to a uniform size before being used for training. I have rescaled them to uniform size of\n",
    "  224 X 224,\n",
    "\n",
    "* For data preprocessing, I have normalized the training dataset and also encoded the labels. I \n",
    "  used torch transforms class to generate augmented images to increase the size of the dataset\n",
    "  and reduce overfitting. I  also resized the images and divided them into training, validation, and\n",
    "  testing sets.\n",
    "\n",
    "* Overall, the project was successful in building a deep learning model to classify images of rice leaf\n",
    "  diseases. I  explored the dataset, preprocessed the data, built and trained deep learning models,\n",
    "  and evaluated their performance. I  also gained insights into how different models behave when\n",
    "  trained on normal vs. augmented data, what makes a good model in terms of accuracy and loss, and\n",
    "  how to analyze models with respect to their time taken per epoch. Finally, I  selected Xception\n",
    "  as the best model and achieved good accuracy on new images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Challenges**\n",
    "* Limited amount of data\n",
    "* Complexity of deep learning models\n",
    "* Hardware limitations\n",
    "* Choosing the best model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
